{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 性能度量\n",
    "\n",
    "**性能度量（performance measure）**指的是用于衡量模型泛化能力的评价标准。使用不同的性能度量往往导致不同的评判结果。比方说搭建推荐系统，两个模型中一个精度高，一个覆盖度高，如果我们想让更多的商品得到推荐可以就会选后一个模型。所以说，模型的好坏是相对的，取决于我们采用什么性能度量，而**采用什么性能度量则应取决于我们的任务需求**。\n",
    "\n",
    "监督学习可分为两类，Classification分类和Regression回归问题，相对应有不同的模型评判标准\n",
    "\n",
    "Classification分类：\n",
    "\n",
    "+ Accuracy准确率\n",
    "+ Precision精确率\n",
    "+ Recall召回率\n",
    "+ F1 score\n",
    "\n",
    "Regression回归：\n",
    "\n",
    "+ Mean Absolute Error平均绝对偏差\n",
    "+ Mean Squared Error均方误差\n",
    "+ $R^2$ score\n",
    "+ Explained Variance Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 分类问题\n",
    "\n",
    "首先讨论分类问题会出现的4种情况\n",
    "\n",
    "1. True positive (TP): 将正类预测为正类，真阳\n",
    "2. False positive (FP): 将负类预测为正类，假阳\n",
    "3. False negative (FN):将正类预测为负类，假阴\n",
    "4. True negative (TN): 将负类预测为负类，真阴\n",
    "\n",
    "这4种情况可用下图表示：\n",
    "\n",
    "![](./img/confusion_matrix.jpg)\n",
    "\n",
    "即True或者False代表与真实数据符合情况，Positive和Negative代表预测情况\n",
    "\n",
    "#### 分类指标\n",
    "\n",
    "**Accuracy准确率**  \n",
    "\n",
    "定义:顾名思义，准确率的定义是对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。即：\n",
    "\n",
    "$$A(M)= \\frac {TN+TP}{TN+FP+FN+TP}$$\n",
    "\n",
    "Accuracy优点很明显，那就是简单直观，但是accuracy有时会陷入Accuracy paradox中。即准确率越高并不代表模型越好。\n",
    "\n",
    "假设某家保险公司需要一个风险评估模型。 这个模型需要检验每个案例是否可能有高风险，当有高风险时，公司会介入调查。现在有一个验证数据集包含10000个案例供检验风险评估模型。这个数据集中有9850个低风险案例（不需要介入调查）和150个高风险案例（需要介入调查）。\n",
    "\n",
    "现在有两个模型，下面是模型1的检验结果：\n",
    "\n",
    "|         |  Predicted Negative   |  Predicted Positive  |\n",
    "| --------   | -----:   | :----: |\n",
    "| Negative Cases  | 9700 | 150 |\n",
    "| Positive Cases | 50 | 100 |\n",
    "\n",
    "通过表格，模型1的的准确率为：\n",
    "\n",
    "$$A(M1)=\\frac{9700+100}{9700+150+50+100}=98.0%$$\n",
    "\n",
    "准确率达到了98%，看起来模型1还很不错。但是由于数据集中本来高风险的案例就很少，如果在模型2中我们将所有案例都预测为低风险，准确率可以达到98.5%，比模型1还要高：\n",
    "\n",
    "|         |  Predicted Negative   |  Predicted Positive  |\n",
    "| --------   | -----:   | :----: |\n",
    "| Negative Cases  | 9850 | 0 |\n",
    "| Positive Cases | 150 | 0 |\n",
    "\n",
    "$$A(M2)=\\frac{9850+0}{9850+150+0+0}=98.5%$$\n",
    "\n",
    "显然模型2并不能帮助保险公司减少保险欺诈的风险。\n",
    "\n",
    "**Precision精确率**\n",
    "\n",
    "定义:精确率计算的是所有”正确被检索的item”占所有”实际被检索到的”的比例. 即：\n",
    "\n",
    "$$P(M)=\\frac{TP}{TP+FP}$$\n",
    "\n",
    "在上面保险公司的例子中，模型1的精确率为\n",
    "\n",
    "$$P(M1)=\\frac{100}{100+150}=40%$$\n",
    "\n",
    "而模型2的精确率就为0了：\n",
    "\n",
    "$$P(M2)=\\frac{0}{0+0}=0%$$\n",
    "\n",
    "**Recall 召回率**\n",
    "\n",
    "召回率计算的是所有”正确被检索的item”占所有”应该检索到的item”的比例。即：\n",
    "\n",
    "$$R(M)=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "在上面保险公司的例子中，模型1的召回率为\n",
    "\n",
    "$$R(M1)=\\frac{100}{100+50}=66.7%$$\n",
    "\n",
    "**F1 score**\n",
    "\n",
    "F1 值是精确率和召回率的调和均值，定义为：\n",
    "\n",
    "$$\\frac{2}{F1}=\\frac{1}{P}+\\frac{1}{R}$$\n",
    "\n",
    "即：\n",
    "\n",
    "$$F1=\\frac{2TP}{2TP+FP+FN}$$\n",
    "\n",
    "precision和recall是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下precision高、recall就低，recall低、precision高，当然如果两者都低，那是什么地方出问题了。当precision和recall都高时，F1的值也会高。在两者都要求高的情况下，可以用F1来衡量。\n",
    "\n",
    "![](./img/f1.png)\n",
    "\n",
    "\n",
    "下面是两个场景：\n",
    "\n",
    "+ 地震的预测:对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。 这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次\n",
    "+ 嫌疑人定罪:基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。\n",
    "\n",
    "#### ROC/AUC/PRC\n",
    "\n",
    "**ROC，全称受试者工作特征（Receiver Operating Characteristic）**。怎样画ROC曲线呢？先定义两个重要的计算量：**真正例率（True Positive Rate，简称TPR）**和**假正例率（False Positive Rate，简称FPR）**。\n",
    "\n",
    "$$TPR = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$FPR = \\frac{FP}{TP+FN}$$\n",
    "\n",
    "TPR其实就等于召回率。在绘制ROC曲线时，纵轴为TPR，横轴为FPR。首先按预测值对样本进行排序，然后按序逐个把样本预测为正例，并计算此时的TPR和FPR，然后在图上画出该点，并与前一个点连线。如下图：\n",
    "\n",
    "![ROC curve](http://www.unc.edu/courses/2010fall/ecol/563/001/images/lectures/lecture22/fig4.png)\n",
    "\n",
    "\n",
    "若一个模型的ROC曲线完全包住了另一个模型的ROC曲线，我们就认为这个模型更优。但是如果两条曲线发生交叉，要怎么判断呢？比较合理的判据是**AUC（Area Under ROC Curve）**，即ROC曲线下的面积。\n",
    "\n",
    "$$AUC=\\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}-x_i)\\cdot(y_i+y_{i+1})$$\n",
    "\n",
    "\n",
    "一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。\n",
    "\n",
    "![](./img/roc.jpg)\n",
    "\n",
    "很多时候，使用模型对测试样本进行预测得到的是一个实值或者概率（比如神经网络），需要进一步设置**阈值（threshold）**，然后把预测值和阈值进行比较才能获得最终预测的标记。\n",
    "\n",
    "我们可以按照预测值对所有测试样本进行排序，最可能是正例的排前面，最不能是正例的排后面。这样分类时就像是在这个序列中以某个**截断点（cut point）**把样本分成两部分。我们需要**根据任务需求来设置截断点**。比如广告推荐更重视查准率，可能就会把截断点设置得更靠前。\n",
    "\n",
    "因此！**排序本身的质量很能体现出一个模型的泛化性能**，ROC曲线就是一个用来衡量排序质量的工具。\n",
    "\n",
    "\n",
    "**precision recall curve**\n",
    "\n",
    "和ROC一样，先看平滑不平滑（蓝线明显好些），在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（绿线比红线好）。F1（计算公式略）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好象AUC对于ROC一样。一个数字比一条线更方便调模型。\n",
    "\n",
    "![](./img/prc)\n",
    "\n",
    "对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义。\n",
    "\n",
    "如果只能选一个指标的话，肯定是选PRC了。可以把一个模型看的一清二楚\n",
    "\n",
    "在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。\n",
    "\n",
    "![](./img/roc_prc.png)\n",
    "\n",
    "这两个分类器都接近完美(非常接近左上角)。图b对应着相同分类器的PR space。而从图b可以看出，这两个分类器仍有巨大的提升空间。\n",
    "那么原因是什么呢？ 通过看Algorithm1的点 A，可以得出一些结论。首先图a和b中的点A是相同的点，只是在不同的空间里。因为TPR=Recall=TP/(TP+FN)，换言之，真阳性率(TPR)和召回率(Recall)是同一个东西，只是有不同的名字。所以图a中TPR为0.8的点对应着图b中Recall为0.8的点。\n",
    "\n",
    "假设数据集有100个positive instances。由图a中的点A，可以得到以下结论：\n",
    "\n",
    "TPR=TP/(TP+FN)=TP/actual positives=TP/100=0.8，所以TP=80\n",
    "\n",
    "由图b中的点A，可得：\n",
    "\n",
    "Precision=TP/(TP+FP)=80/(80+FP)=0.05，所以FP=1520\n",
    "\n",
    "再由图a中点A，可得：\n",
    "\n",
    "FPR=FP/(FP+TN)=FP/actual negatives=1520/actual negatives=0.1，所以actual negatives是15200。\n",
    "\n",
    "由此，可以得出原数据集中只有100个positive instances，却有15200个negative instances！这就是极不均匀的数据集。直观地说，在点A处，分类器将1600 (1520+80)个instance分为positive，而其中实际上只有80个是真正的positive。 我们凭直觉来看，其实这个分类器并不好。但由于真正negative instances的数量远远大约positive，ROC的结果却“看上去很美”。所以在这种情况下，PRC更能体现本质。\n",
    "\n",
    "结论： 在negative instances的数量远远大于positive instances的data set里， PRC更能有效衡量分类器的好坏。\n",
    "\n",
    "References:Davis, Jesse, and Mark Goadrich. \"The relationship between Precision-Recall and ROC curves.\" Proceedings of the 23rd International Conference on Machine Learning (ICML). ACM, 2006\n",
    "\n",
    "\n",
    "#### 代价敏感错误率与代价曲线\n",
    "\n",
    "现实任务中，有时会遇到不同类型错误造成后果不同的状况。比如医生误诊，把患者诊断为健康人的影响远大于把健康人诊断为患者，因为可能因为这次误诊丧失了最佳治疗时机。为了权衡不同类型错误带来的不同损失，可以为这些错误类型赋以**非均等代价（unequal cost）**。\n",
    "\n",
    "还是举二分类为类，可以**根据任务的领域知识**来设定一个**代价矩阵（cost matrix）**:\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "    <th rowspan=\"2\" align=\"center\">真实类别</th>\n",
    "    <th colspan=\"2\" align=\"center\">预测类别</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td align=\"center\">第0类</td>\n",
    "    <td align=\"center\">第1类</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td align=\"center\">第0类</td>\n",
    "    <td align=\"center\">0</td>\n",
    "    <td align=\"center\">$cost_{01}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td align=\"center\">第1类</td>\n",
    "    <td align=\"center\">$cost_{10}$</td>\n",
    "    <td align=\"center\">0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "预测值与真实值相等时，自然错误代价为0。但把第0类错预测为第1类和把第1类错预测为第0类这两种错误的代价是不同的。注意，**重要的不是代价在数值上的大小，而是它们的比值**。比方说 $\\frac{cost_{01}}{cost_{10}} > 1$， 这就说明把第0类错预测为第1类的代价更高。\n",
    "\n",
    "使用了非均等代价之后，我们在使用性能度量时自然也需要作出相应的改变，比方说**代价敏感（cost-sensitive）**版本的错误率：\n",
    "\n",
    "$$E(f;D;cost) = \\frac{1}{m}\\lgroup\\sum_{x_i \\in D^+}\\mathbb{I}(f(x_i) \\neq y_i) \\times cost_{01} + \\sum_{x_i \\in D^-}\\mathbb{I}(f(x_i) \\neq y_i) \\times cost_{10}\\rgroup$$\n",
    "\n",
    "由于ROC曲线不能反应使用非均等代价之后的期望总体代价，所以改用**代价曲线（cost curve）**来取替。\n",
    "\n",
    "代价曲线图的纵轴为归一化代价（将代价映射到 [0,1] 区间），横轴为正例概率代价。画法类似于ROC曲线，它是将ROC曲线的每一个点转为图中的一条线。依次计算出ROC曲线每个点对应的FPR和FNR，然后把点 (0,FPR) 和点 (0,FNR) 连线。最终所得的图中，所有线的下界所围成的面积就是该模型的期望总体代价。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 回归问题度量\n",
    "\n",
    "#### Mean Absolute Error(MAE)\n",
    "\n",
    "MAE用来描述预测值和真实值的差值。数值越小越好。\n",
    "\n",
    "假设$y_i$是真实值，$f_i$ 是相对应的预测值，则$n$ 个样本的MAE可由下式出给：\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n}|f_i-y_i|$$\n",
    "\n",
    "#### Mean Squared Error(MSE)\n",
    "\n",
    "Mean Squared Error也称为Mean Squared Deviation(MSD)，计算的是预测值和实际值的平方误差。同样，数值越小越好。\n",
    "\n",
    "假设$y_i$是真实值，$f_i$ 是相对应的预测值，则$n$ 个样本的MSE由下式公式给出：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(f_i - y_i)^2$$\n",
    "\n",
    "MSE与Bias和Variance的关联\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\mathbb {E} \\left[({\\hat {\\theta }}-\\theta )^{2}\\right].}$$\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {Var} ({\\hat {\\theta }})+\\operatorname {Bias} ({\\hat {\\theta }},\\theta )^{2}.}$$\n",
    "\n",
    "#### Explained Variance Score\n",
    "\n",
    "假设$y$是真实值，$f$ 是相对应的预测值，VarVar是方差，Explained Variance由下式公式给出\n",
    "\n",
    "$$Explained\\ Variance = 1 - \\frac{Var(y - f)}{Var(y)}$$\n",
    "\n",
    "值最大为1，越接近1越好\n",
    "\n",
    "\n",
    "#### $R^2$ Score\n",
    "\n",
    "$R^2$ Score又称为the coefficient of determination。判断的是预测模型和真实数据的拟合程度，最佳值为1，同时可为负值。\n",
    "\n",
    "假设$y_i$是真实值，$f_i$ 是相对应的预测值，则nn 个样本的$R^2$ score由下式公式给出：\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - f_i)^2}{\\sum_{i=1}^{n}(y_i - \\overline{y})^2}$$\n",
    "\n",
    "其中$\\overline{y}$ 是$y$的均值$\\overline{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
