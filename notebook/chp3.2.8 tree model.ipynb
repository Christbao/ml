{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树\n",
    "\n",
    "决策树是通过一系列规则对数据进行分类的过程。它提供一种在什么条件下会得到什么值的类似规则的方法。决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。\n",
    "\n",
    "近来的调查表明决策树也是最经常使用的数据挖掘算法，它的概念非常简单。决策树算法之所以如此流行，一个很重要的原因就是使用者基本上不用了解机器学习算法，也不用深究它是如何工作的。直观看上去，决策树分类器就像判断模块和终止块组成的流程图，终止块表示分类结果（也就是树的叶子）。判断模块表示对一个特征取值的判断（该特征有几个值，判断模块就有几个分支）。\n",
    "\n",
    "如果不考虑效率等，那么样本所有特征的判断级联起来终会将某一个样本分到一个类终止块上。实际上，样本所有特征中有一些特征在分类时起到决定性作用，决策树的构造过程就是找到这些具有决定性作用的特征，根据其决定性程度来构造一个倒立的树--决定性作用最大的那个特征作为根节点，然后递归找到各分支下子数据集中次大的决定性特征，直至子数据集中所有数据都属于同一类。所以，构造决策树的过程本质上就是根据数据特征将数据集分类的递归过程，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。\n",
    "\n",
    "为了找到决定性的特征、划分出最好的结果，我们必须评估数据集中蕴含的每个特征，寻找分类数据集的最好特征。完成评估之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则则该分支处理完成，称为一个叶子节点，即确定了分类。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内（叶子节点）\n",
    "\n",
    "![](img/tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  决策树的学习过程\n",
    "\n",
    "一棵决策树的生成过程主要分为以下3个部分:\n",
    "\n",
    "1. 特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。\n",
    "\n",
    "2. 决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。\n",
    "\n",
    "3. 剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树算法\n",
    "\n",
    "典型决策树有ID3、CART和C4.5等算法，其中C4.5和CART两种算法从ID3算法中衍生而来。\n",
    "\n",
    "### ID3\n",
    "\n",
    "Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息论的信息增益评估和选择特征，每次选择信息增益最大的特征做判断模块。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性--就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。\n",
    "\n",
    "### C4.5\n",
    "\n",
    "是ID3的一个改进算法，继承了ID3算法的优点。C4.5算法用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不完整数据进行处理。C4.5算法产生的分类规则易于理解、准确率较高；但效率低，因树构造过程中，需要对数据集进行多次的顺序扫描和排序。也是因为必须多次数据集扫描，C4.5只适合于能够驻留于内存的数据集。\n",
    "\n",
    "### CART\n",
    "\n",
    "算法的全称是Classification And Regression Tree，采用的是Gini指数（选Gini指数最小的特征s）作为分裂标准,同时它也是包含后剪枝操作。ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，就出现了根据GINI系数来选择测试属性的决策树算法CART。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 算法原理\n",
    "\n",
    "### 信息熵\n",
    "在概率论中，信息熵给了我们一种度量不确定性的方式，是用来衡量随机变量不确定性的，熵就是信息的期望值。若待分类的事物可能划分在$N$类中，分别是$x_1，x_2，……，x_n$，每一种取到的概率分别是$P_1，P_2，……，P_n$，那么$X$的熵就定义为：\n",
    "\n",
    "$$H(X)=-\\sum_{i=1}^n p_i logp_i$$\n",
    "\n",
    "从定义可知$0\\le H(X) \\le log(n)$\n",
    "\n",
    "当随机变量只取两个值时，即X的分布为 $P(X=1)=p,X(X=0)=1−p,0 \\le p \\le 1$则熵为:$H(X)=−plog_2(p)−(1−p)log_2(1−p)$\n",
    "\n",
    "熵值越高，则数据混合的种类越高，其蕴含的含义是一个变量可能的变化越多（反而跟变量具体的取值没有任何关系，只和值的种类多少以及发生概率有关），它携带的信息量就越大。熵在信息论中是一个非常重要的概念，很多机器学习的算法都会利用到这个概念\n",
    "\n",
    "### 条件熵\n",
    "\n",
    "假设有随机变量(X,Y)，其联合概率分布为:$P(X=x_i,Y=y_j)=p_{ij},i=1,2,⋯,n;j=1,2,⋯,m$\n",
    "\n",
    "则条件熵($H(Y∣X)$)表示在已知随机变量X的条件下随机变量Y的不确定性，其定义为X在给定条件下Y的条件概率分布的熵对X的数学期望:\n",
    "\n",
    "$$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)$$\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "信息增益(information gain)表示得知特征X的信息后，而使得Y的不确定性减少的程度。定义为:\n",
    "\n",
    "$$ g(D,A)=H(D)-H(D|A) $$ \n",
    "\n",
    "### ID3算法\n",
    "\n",
    "+  信息熵  \n",
    "假设一个分类系统的样本空间（D,Y），D表示样本（有m个特征），Y表示n个类别，可能的取值是$C_1，C_2，...，C_n$。每一个类别出现的概率是$P(C_1)，P(C$2)，...，P(C$n)$。该分类系统的熵为：\n",
    "\n",
    "$$H(C)=\\sum_{i=1}^nP(C_i)log_2P(C_i)$$\n",
    "\n",
    "离散分布中，类别$C_i$出现的概率$P(C_i)$，通过该类别出现的次数除去样本总数即可得到。对于连续分布，常需要分块做离散化处理获得\n",
    "\n",
    "+ 条件熵\n",
    "\n",
    "根据条件熵的定义,分类系统中的条件熵指的是当样本的某一特征X固定时的信息熵。由于该特征X可能的取值会有$x_1，x_2，……，x_n）$，当计算条件熵而需要把它固定的时候，每一种可能都要固定一下，然后求统计期望。因此样本特征$X$取值为$x_i$的概率是Pi，该特征被固定为值$x_i$时的条件信息熵就是$H(C|X=x_i)$，那么$H(C|X)$就是分类系统中特征$X$被固定时的条件熵$（X=（x_1，x_2，……，x_n））$：\n",
    "\n",
    "$$H(C|X)=P_1H(C|X=x_1)+P_2H(C|X=x_2)+...+P_nH(C|X=x_n)=\\sum_{i=1}^nP_iH(C|X=x_1)$$\n",
    "\n",
    "+ 信息增益\n",
    "\n",
    "根据信息增益的公式，分类系统中特征X的信息增益就是：$Gain(D, X) = H(C)-H(C|X)$\n",
    "\n",
    "信息增益是针对一个一个的特征而言的，就是看一个特征X，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息增益。每次选取特征的过程都是通过计算每个特征值划分数据集后的信息增益，然后选取信息增益最高的特征。\n",
    "\n",
    "+ 循环\n",
    "\n",
    "经过上述一轮信息增益计算后会得到一个特征作为决策树的根节点，该特征有几个取值，根节点就会有几个分支，每一个分支都会产生一个新的数据子集Dk，余下的递归过程就是对每个Dk再重复上述过程，直至子数据集都属于同一类。\n",
    "\n",
    "在决策树构造过程中可能会出现这种情况：所有特征都作为分裂特征用光了，但子集还不是纯净集（集合内的元素不属于同一类别）。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5\n",
    "\n",
    "以信息增益进行分类决策时，存在偏向于取值较多的特征的问题。于是为了解决这个问题于是有了基于信息增益比的分类决策方法，也就是C4.5。C4.5与ID3都是利用贪婪算法进行求解，不同的是分类决策的依据不同。\n",
    "\n",
    "因此，C4.5算法在结构与递归上与ID3完全相同，区别就在于选取决断特征时选择信息增益比最大的。\n",
    "\n",
    "信息增益比率度量是用ID3算法中的的增益度量$Gain(D，X)$和分裂信息度量$SplitInformation(D，X)$来共同定义的。分裂信息度量$SplitInformation(D，X）$就相当于特征$X$（取值为$x_1，x_2，……，x_n$，各自的概率为$P_1，P_2，...，P_n$，$P_k$就是样本空间中特征$X$取值为$x_k$的数量除上该样本空间总数）的熵。\n",
    "\n",
    "$$SplitInformation(D，X） = -P_1 log_2(P_1)-P_2 log_2(P_$)-,...,-P_n log2(P_n)$$\n",
    "\n",
    "$$GainRatio(D,X) = Gain(D,X)/SplitInformation(D,X)$$\n",
    "\n",
    "在ID3中用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性，在C4.5中由于除以$SplitInformation(D,X)=H(X)$，可以削弱这种作用。\n",
    "\n",
    "C4.5既可以处理离散型属性，也可以处理连续性属性。在选择某节点上的分枝属性时，对于离散型描述属性，C4.5的处理方法与ID3相同。对于连续分布的特征，其处理方法是：\n",
    "\n",
    "先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$<=v_j$的分到左子树，$>v_j$的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。经证明，在决定连续特征的分界点时采用增益这个指标（因为若采用增益率，splittedinfo影响分裂点信息度量准确性，若某分界点恰好将连续特征分成数目相等的两部分时其抑制作用最大），而选择属性的时候才使用增益率这个指标能选择出最佳分类特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 剪枝\n",
    "\n",
    "分析分类回归树的递归建树过程，不难发现它实质上存在着一个数据过度拟合问题。在决策树构造时，由于训练数据中的噪音或孤立点，许多分枝反映的是训练数据中的异常，使用这样的判定树对类别未知的数据进行分类，分类的准确性不高。因此试图检测和减去这样的分支，检测和减去这些分支的过程被称为树剪枝。树剪枝方法用于处理过分适应数据问题。通常，这种方法使用统计度量，减去最不可靠的分支，这将导致较快的分类，提高树独立于训练数据正确分类的能力。\n",
    "\n",
    "决策树常用的剪枝常用的简直方法有两种：预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。预剪枝是根据一些原则及早的停止树增长，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的最大幅度小于用户指定的幅度等。预剪枝的核心问题是如何事先指定树的最大深度，如果设置的最大深度不恰当，那么将会导致过于限制树的生长，使决策树的表达式规则趋于一般，不能更好地对新数据集进行分类和预测。除了事先限定决策树的最大深度之外，还有另外一个方法来实现预剪枝操作，那就是采用检验技术对当前结点对应的样本集合进行检验，如果该样本集合的样本数量已小于事先指定的最小允许值，那么停止该结点的继续生长，并将该结点变为叶子结点，否则可以继续扩展该结点。\n",
    "\n",
    "后剪枝则是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点，可以使用的后剪枝方法有多种，比如：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。后剪枝操作是一个边修剪边检验的过程，一般规则标准是：在决策树的不断剪枝操作过程中，将原样本集合或新数据集合作为测试数据，检验决策树对测试数据的预测精度，并计算出相应的错误率，如果剪掉某个子树后的决策树对测试数据的预测精度或其他测度不降低，那么剪掉该子树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "分类和回归树 (CART) 包含了上述两种决策树, 最先由Breiman 等提出.分类树和回归树有些共同点和不同点—例如处理在何处分裂的问题。\n",
    "\n",
    "分类回归树(CART,Classification And Regression Tree)也属于一种决策树，前面我们介绍了基于ID3和C4.5算法的决策树。\n",
    "\n",
    "### CART与ID3区别\n",
    "\n",
    "+ CART中用于选择变量的不纯性度量是Gini指数；\n",
    "+ 如果目标变量是标称的，并且是具有两个以上的类别，则CART可能考虑将目标类别合并成两个超类别（双化）；\n",
    "+ 如果目标变量是连续的，则CART算法找出一组基于树的回归方程来预测目标变量。\n",
    "\n",
    "### GINI指数\n",
    "\n",
    "1. 是一种不等性度量；\n",
    "2. 通常用来度量收入不平衡，可以用来度量任何不均匀分布；\n",
    "3. 是介于0~1之间的数，0-完全相等，1-完全不相等；\n",
    "4. 总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似)\n",
    "\n",
    "在CART算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。\n",
    "\n",
    "假设$y$的可能取值为${1, 2, ..., m}$,令$f_i$是样本被赋予$i$的概率，则基尼指数可以通过如下计算：\n",
    "\n",
    "$$I_G(f)=\\sum_{i=1}^mf_i(1-f_i)=1-\\sum_{i=1}^mf_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
