{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性分类器\n",
    "\n",
    "对输入的m 个训练样本${{\\left\\{ {{x}^{i}},{{y}^{i}} \\right\\}}_{i=1\\cdots m}},{{x}^{i}}\\in {{\\Re }^{n}},{{y}^{i}}\\in \\{-1,1\\}$ ，线性分类器希望找到一个超平面$w^Tx+b=0$ 将两类样本分开，使得对于样本类别$y_i=1$ 的样本$w^Tx_i+b>0$ ；对于样本类别$y_i=−1$的样本$w^Tx_i+b<0$。这样，超平面$w^Tx+b=0$便是一个“完美”的分类器。需要注意，这里这个“完美”的分类器要求数据是要线性可分的，对于非线性可分我们后面再讨论。\n",
    "\n",
    "![](img/svm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大间隔分类器\n",
    "如果数据是线性可分的，那么一般存在多组$(w,b)$是的超平面$w^Tx+b=0$可以将数据正确分类，那么就需要一个标准来从中选取一个最优的\n",
    "![](img/svm-sep.jpg)\n",
    "\n",
    "最大间隔分类器以几何间隔为标准，从中选取离所有样本的集合的几何间隔最大的一个分类超平面作为最优分类超平面，因此我们将问题转化为最大化间距，也就是那个m。我们的SVM目标就是解决这个问题，而\n",
    "\n",
    "$$m=\\frac{2}{\\left\\| w \\right\\|}$$\n",
    "\n",
    "更详细内容可以参考\n",
    "\n",
    "http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/  \n",
    "http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/  \n",
    "http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/  \n",
    "\n",
    "\n",
    "因此最大化间距就可以进一步转化为\n",
    "\n",
    "$$\\begin{align}\\left\\{ \\begin{matrix}\\underset{w,b}{\\mathop{\\min }}\\,\\frac{1}{2}{{\\left\\| w \\right\\|}^{2}}  \\\\\\begin{matrix}s.t. & {{y}^{i}}({{w}^{T}}{{x}^{i}}+b)\\ge 1  \\\\\\end{matrix}  \\\\\\end{matrix} \\right.\\end{align}$$\n",
    "\n",
    "## 对偶问题\n",
    "\n",
    "SVM的基础——最大间隔分类器就形式化为以上优化问题，而且不难发现它还是个凸优化问题，求解很方便。但是，我们并不直接求解这个问题，而是转而求解它的对偶问题,首先构建拉格朗日函数\n",
    "\n",
    "$$\\begin{align}L(w,b,\\alpha )=\\frac{1}{2}{{\\left\\| w \\right\\|}^{2}}+\\sum\\limits_{i}{{{\\alpha }_{i}}(1-{{y}^{i}}({{w}^{T}}{{x}^{i}}+b))}\\end{align}$$\n",
    "\n",
    "$$\\begin{align*}\\frac{\\partial L(w,b,\\alpha )}{\\partial w}&=\\frac{\\partial \\left[ \\frac{1}{2}{{\\left\\| w \\right\\|}^{2}}+\\sum\\limits_{i}{{{\\alpha }_{i}}(1-{{y}^{i}}({{w}^{T}}{{x}^{i}}+b))} \\right]}{\\partial w} \\\\& =\\frac{\\partial \\left[ \\frac{1}{2}{{w}^{T}}w+\\sum\\limits_{i}{{{\\alpha }_{i}}(1-{{y}^{i}}({{w}^{T}}{{x}^{i}}+b))} \\right]}{\\partial w} \\\\& =w-\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}{{x}^{i}}} \\\\\\end{align*}$$\n",
    "\n",
    "定义 ${{\\theta }_{D}}(\\alpha )=\\underset{w,b}{\\mathop{\\min }}\\,L(w,b,\\alpha )$则前面的对偶问题可以写成\n",
    "\n",
    "$$\\begin{align}\\underset{\\alpha }{\\mathop{\\max }}\\,\\underset{w,b}{\\mathop{\\min }}\\,L(w,b,\\alpha )=\\underset{\\alpha }{\\mathop{\\max }}\\,{{\\theta }_{D}}(\\alpha )\\end{align}$$\n",
    "\n",
    "通过对w,b求偏导，如何计算可以求解为\n",
    "\n",
    "$$\\begin{align}\\left\\{ \\begin{matrix}\\underset{\\alpha }{\\mathop{\\max }}\\,\\sum\\limits_{i}{{{\\alpha }_{i}}}-\\frac{1}{2}\\sum\\limits_{i,j}{{{\\alpha }_{i}}{{\\alpha }_{j}}{{y}^{i}}{{y}^{j}}<{{x}^{i}},{{x}^{j}}>}  \\\\s.t.\\left\\{ \\begin{matrix}{{\\alpha }_{i}}\\ge 0  \\\\\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}}=0  \\\\\\end{matrix} \\right.  \\\\\\end{matrix} \\right.\\end{align}$$\n",
    "\n",
    "其中约束${{\\alpha }_{i}}\\ge 0$是拉格朗日对偶问题本身的要求，约束$\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}}=0$代表$\\frac{\\partial L(w,b,\\alpha )}{\\partial b}=0$的结果\n",
    "\n",
    "对偶问题求解的结果是得到$\\alpha$ 的取值。当$\\alpha$得到解后，就可以根据$w=\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}{{x}^{i}}}$解出$w$ 。$w$确定了分类超平面的方向，$b$ 使得超平面有一个平移，根据最大间隔分类器的准则，最优超平面是穿过两类样本“最中间”的一个平面，所以\n",
    "\n",
    "$$\\begin{align}b=-\\frac{\\underset{i:{{y}^{i}}=-1}{\\mathop{\\max }}\\,{{w}^{T}}{{x}^{i}}+\\underset{i:{{y}^{i}}=1}{\\mathop{\\min }}\\,{{w}^{T}}x}{2}\\end{align}$$\n",
    "\n",
    "$(w,b)$ 确定后，分类器就确定了，就是超平面$w^Tx+b=0$ ，对于新的输入样本$x$ ，如果$w^Tx+b>0$则判别它样本类别为1，否则判别它样本类别为-1。把$w$带入判别函数\n",
    "\n",
    "$$\\begin{align}{{w}^{T}}x+b=\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}<{{x}^{i}},x>}+b\\end{align}$$\n",
    "\n",
    "判别式写成了向量内积的形式，看似需要计算输入x与所有训练样本的内积，但实际上还可以简化。\n",
    "\n",
    "## 支持向量\n",
    "\n",
    "支持向量就是函数间隔为1的样本，它们也是所有样本中函数间隔最小的样本。最优分类超平面（红色）和对于的函数间隔为1的样本（两条黑线上的样本），对左侧黑线上的支持向量有$w^Txl+b=−1$ ，对于右侧黑线上的支持向量有$w^Txr+b=1$\n",
    "![](img/svm-vec.jpg)\n",
    "\n",
    "## 核函数\n",
    "\n",
    "前面的优化问题虽然可以直接求解，但是要基于训练数据线性可分的基础，如果数据本身线性不可分呢？解决方法之一就是将数据，或者更加正式的称为特征，向高维映射，以期待映射后数据在更高维的空间中数据可分，这样就可以在新的高维空间中继续使用之前的方法\n",
    "\n",
    "![](img/kernel.jpg)\n",
    "\n",
    "要对上面的一维数据分类,我们定义$\\phi (x)=(x,{{x}^{2}})$将原始的一维数据映射到二维，数据变的线性可分了。这种向高维映射的方法一定程度上解决的线性不可分问题，但同时也带来了相当的计算复杂度。\n",
    "\n",
    "SVM现在将在新的二维空间运行，原来问题中的$x$ 将用$\\phi(x)$ 取代，优化问题中向量内积$<x^i,x^j>$ 也将被$<\\phi ({{x}^{i}}),\\phi ({{x}^{j}})>$ 取代，判别函数中向量内积$<x^i,x>$也将被$<\\phi ({{x}^{i}}),\\phi (x)>$取代，换句话说，我们需要将原来所有一维训练样本首先一个个计算，映射到新的二维空间，然后在二维空间中计算新样本之间的内积。这与原来直接在一维空间中直接计算一维向量内积相比，虽然并不会复杂多少，然而随着样本维度和映射后维度的增加，计算复杂的的提升是非常显著的，甚至变得不可计算(高斯核将样本映射的无限维空间)\n",
    "\n",
    "上面的过程是由两个步骤组成的。首先是映射，其次是计算映射后的内积。但其实我们真正需要的知道的只是映射后的内积$<\\phi ({{x}^{i}}),\\phi ({{x}^{j}})>$和$<\\phi ({{x}^{i}}),\\phi (x)>$。 对优化问题和判别函数，怎么绕过映射本身而直接求解映射后向量内积呢？核函数就是用来做这样的事情\n",
    "\n",
    "$$\\begin{align}K(x,y)=<\\phi (y),\\phi (y)>\\end{align}$$\n",
    "\n",
    "以上核函数的定义应该来讲是“意义化”的，而不是“计算化”的，定义中出现$\\phi (x)$,$\\phi (y)$并不表明需要显式计算映射本身，而是来说明核函数的意义是映射后向量的内积，而核函数“计算化”的定义是需要我们自己设计给出的。\n",
    "\n",
    "以前面例子$\\phi (x)=(x,{{x}^{2}})$,$\\phi (y)=(y,{{y}^{2}})$\n",
    "$$\\begin{align*}<\\phi (x),\\phi (y)> & ={{(x,{{x}^{2}})}^{T}}(y,{{y}^{2}}) \\\\& =xy+{{(xy)}^{2}} \\\\\\end{align*}$$\n",
    "\n",
    "那么我们就可以定义\n",
    "$$K(x,y)=xy+{{(xy)}^{2}}$$\n",
    "\n",
    "这和映射到二维后再计算内积的结果是一样的，但是现在直接计算$K(x,y)=xy+(xy)^2$使得我们不需要关心映射本身，而且计算是在原来的一维空间进行的。当样本维度增加时，这样方法带来的计算优势是不言而喻的。这就是核函数的主要作用，它隐藏映射本身而直接在低维空间计算高维空间向量的内积，显著降低计算复杂度。这简直就像加了特技，duang一下就完成了高维空间内积的计算，所以有些文献将核函数称为kernel trick，是trick，是特技!\n",
    "\n",
    "高斯核  $K(x,y)=\\exp (-\\frac{{{\\left\\| x-y \\right\\|}^{2}}}{2{{\\sigma }^{2}}})$  \n",
    "多项式核 $K(x,y)={{(<x,y>+c)}^{d}}$  \n",
    "感知器核 $K(x,y)=\\tanh ({{\\rho }_{1}}<x,y>+{{\\rho }_{2}})$  \n",
    "\n",
    "些核函数都对应不同的映射，拥有不同的性质。比如高斯核对应一个到无限维空间的映射，而且高斯核的意义比较明确：对原空间内相近的两个向量$x、y$计算 结果的数值较大，反之较小，参数$\\sigma$ 控制这核函数值随${{\\left\\| x-y \\right\\|}^{2}}$变化的速度；多项式核对应一个到$C_{n+d}^{d}$ 维的映射。仅从这两个其他核函数的映射维度来看，核函数在降低计算复杂度上是很显著的，比如对多项式核，可以取n （原样本维度）和d 为不同值，看看映射后维度有多高\n",
    "\n",
    "有了核函数后，新的优化变成\n",
    "$$\\begin{align}\\left\\{ \\begin{matrix}\\underset{\\alpha }{\\mathop{\\max }}\\,\\sum\\limits_{i}{{{\\alpha }_{i}}}-\\frac{1}{2}\\sum\\limits_{i,j}{{{\\alpha }_{i}}{{\\alpha }_{j}}{{y}^{i}}{{y}^{j}}K(x{{,}^{i}}{{x}^{j}})}  \\\\s.t.\\left\\{ \\begin{matrix}{{\\alpha }_{i}}\\ge 0  \\\\\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}}=0  \\\\\\end{matrix} \\right.  \\\\\\end{matrix} \\right.\\end{align}$$\n",
    "\n",
    "判别函数就变成\n",
    "\n",
    "$$\\begin{align}{{w}^{T}}x+b=\\sum\\limits_{i}{{{\\alpha }_{i}}{{y}^{i}}K({{x}^{i}},x)}+b\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性不可分问题\n",
    "\n",
    "\n",
    "通过计算样本核函数，实际上将样本映射到高维空间以望使其线性可分的方法，一定程度上解决了线性不可分问题，但并不彻底。\n",
    "现在，换个思路，对于线性不可分问题不再千方百计的变换数据使其线性可分，对于有些数据，找到合适的变换可能是相当困难的。我们允许数据线性不可分，允许得到的分类器对一些样本而言不“完美”，但分类器得为自己的不“完美”付出代价，它要受到惩罚！\n",
    "\n",
    "$$\\begin{align}\\left\\{ \\begin{matrix}\\underset{w,b}{\\mathop{\\min }}\\,\\frac{1}{2}{{\\left\\| w \\right\\|}^{2}}  \\\\\\begin{matrix}s.t. & {{y}^{i}}({{w}^{T}}{{x}^{i}}+b)\\ge 1  \\\\\\end{matrix}  \\\\\\end{matrix} \\right.\\end{align}$$\n",
    "\n",
    "这里要求所有样本的函数间隔至少为1,现在，我们放松要求，不要求那么苛刻了，对于一些样本并不要求函数间隔至少为1，比1小点也行（分的不够开了），小到小于0了也行（错分了）。写成表达式就是${{y}^{i}}({{w}^{T}}{{x}^{i}}+b)\\ge 1-{{\\xi }_{i}},{{\\xi }_{i}}\\ge 0$，于是优化问题会变化，同时因为${{\\xi }_{i}}$引入，那么我也给你加个惩罚${{C}_{i}}{{\\xi }_{i}}$,这里$C_{i}$就是惩罚因子。优化问题就变成\n",
    "\n",
    "$$\\begin{align}\\left\\{ \\begin{matrix}\\underset{w,b,\\xi }{\\mathop{\\min }}\\,\\frac{1}{2}{{\\left\\| w \\right\\|}^{2}}+C\\sum\\limits_{i}{{{\\xi }_{i}}}  \\\\\\left\\{ \\begin{matrix}\\begin{matrix}s.t. & 1-{{\\xi }_{i}}-{{y}^{i}}({{w}^{T}}{{x}^{i}}+b)\\le 0  \\\\\\end{matrix}  \\\\ -{{\\xi }_{i}}\\le 0  \\\\\\end{matrix} \\right.  \\\\\\end{matrix} \\right.\\end{align}$$\n",
    "\n",
    "核函数通过向高维映射来解决线性不可分问题，现在有引人了惩罚机制了解决线性不可分问题问题，但不能简单地将它们看作完全独立的方法。如果仅仅依赖于惩罚机制，在样本线性不可分的情况下得到是所谓最优分类器其实也是比较差的，这是由样本本身的分布决定的，所以还是需要首先用“核函数完成映射”，使样本变得尽可能线性可分。\n",
    "\n",
    "另外，惩罚机制也解决了噪声问题，通过对噪声数据较小的惩罚损失来换取目标函数整体上较大的优化。\n",
    "\n",
    "![](img/svm-kernel.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
