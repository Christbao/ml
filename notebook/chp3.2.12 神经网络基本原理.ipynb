{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 神经网络简史\n",
    "\n",
    "### 1940s-1960s \n",
    "\n",
    "**线性模型**\n",
    "\n",
    "$$f(x,w)=x_1W_1+x_2W_2+...+x_nw_n$$\n",
    "\n",
    "+ (1943) McCulloch-Pitts neuron: served as a computational model; weights are set by operator\n",
    "+ (1957) Rosenblatt’s percepton: algorithm to learn the weights from input; was used in a machine intended for image recognition\n",
    "+ (1960) Widrow and Hoff’s adaptive linear neuron: trained using a special case of stochastic gradient descent\n",
    "\n",
    "### 1980s-1990s\n",
    "\n",
    "+ Distributed representation: the inputs should have many features, and the features should be representative of many inputs\n",
    "+ Backpropagation: Algorithm to train neural networks. Is still the dominant approach today.\n",
    "\n",
    "### 2006-\n",
    "\n",
    "+ Increasing data set sizes.\n",
    ">“Deep learning will generally... match or exceed human performance.. with 10 million labelled examples” (Goodfellow et al) \n",
    "+ Increasing model sizes. Models double in size every ~2.4 years.\n",
    "+ Increasing accuracy and complexity. Significant drops in the error rate for object and speech recognition problems using NNs.\n",
    "\n",
    "**1940s -** The beginning of Neural Networks (Electronic Brain)  \n",
    "**1950s and 1960s** - The first golden age of Neural Networks (Perceptron)  \n",
    "**1970s -** The winter of Neural Networks (XOR problem)  \n",
    "**1980s -** Renewed enthusiasm (Multilayered Perceptron, backpropagation)  \n",
    "**1990s -** Subfield of Radial Basis Function Networks was developed  \n",
    "**2000s -** The power of Neural Networks Ensembles & Support Vector Machines is apparent  \n",
    "**2006 -** Hinton presents the Deep Belief Network (DBN)  \n",
    "**2009 -** Deep Recurrent Neural Network  \n",
    "**2010 -** Convolutional Deep Belief Network (CDBN)  \n",
    "**2011 -** Max-Pooling CDBN  \n",
    "\n",
    "![](./img/figure1_ANN_history.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的直观理解\n",
    "\n",
    "![](./img/neuron.png)\n",
    "**左图 : 生物神经元（neuron）**: 人类的神经系统中大约有860亿个神经元，它们被大约$10^{14}-10^{15}$个突触（synapses）连接起来, 每个神经元都从它的树突获得输入信号，然后沿着它唯一的轴突（axon）产生输出信号, 轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。\n",
    "\n",
    "**右图 : 神经元数学模型**: 轴突传播的信号($x_0$\n",
    ")是基于轴突对信号的敏感程度($w_0$)的, 突触的强度(权重向量w)是可以学习的并且控制着和其他神经元的连接强度, 这种连接强度可以表现为对信号的兴奋或者抑制, 在细胞体(cell body)中, 将所有的加权信号相加并和一个阈值进行比较, 从而得到输出信号, 在这个模型中假设峰值信号的准确时间点不重要，是通过激活信号的激活频率在交流信息, 将神经元的激活率建模为激活函数f（activation function）, 通常采用Sigmoid函数, 该函数会将求和之后的信号映射到(0,1)之间\n",
    "\n",
    "以识别奥巴马和谢耳朵的图像为例\n",
    "\n",
    "**信息在网络中的传递**\n",
    "\n",
    "![](./img/figure7_ANN_example_1.jpg)\n",
    "\n",
    "**信息的反馈**\n",
    "\n",
    "![](./img/figure8_ANN_example_2.jpg)\n",
    "\n",
    "**权重更新**\n",
    "![](./img/figure9_ANN_example_3.jpg)\n",
    "\n",
    "**正确识别图像**\n",
    "![](./img/figure10_ANN_example_4.jpg)\n",
    "\n",
    "神经网络的权重更新可视化\n",
    "\n",
    "http://www.emergentmind.com/neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络基本原理\n",
    "\n",
    "单层神经元\n",
    "\n",
    "\n",
    "![](./img/simple-nn.png)\n",
    "\n",
    "计算过程\n",
    "\n",
    "![](./img/simple-nn-0.png)\n",
    "\n",
    "![](./img/simple-nn-1.png)\n",
    "\n",
    "多层神经网络\n",
    "\n",
    "![](./img/multi_layer.png)\n",
    "\n",
    "![](./img/multi_layer_w.png)\n",
    "\n",
    "假设我们记  \n",
    "\n",
    "$$w^l=[w^l_{ij}]$$\n",
    "\n",
    "+ $w^l$中的行数代表了 $l$层的神经元数目\n",
    "+ $w^l$ 列代表了$l-1$层的神经元数目\n",
    "+ 那么$w^l_{ij}$就是作用于第$l-1$层的第j个神经元权重，它用于计算$l$层的神经元\n",
    "\n",
    "\n",
    "    class Network(object):\n",
    "      def __init__(self, sizes):\n",
    "      self.num_layers = len(sizes)\n",
    "      self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "      self.weights = [np.random.randn(y, x)\n",
    "      for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "例如Network([3, 4, 4, 1])则是初始化一个有3个输入，2个隐藏层，每层4个神经元，1个输出的神经网络\n",
    "\n",
    "### 目标函数\n",
    "\n",
    "假设为MSE\n",
    "\n",
    "$$ E=\\frac{1}{2m}\\sum_x||y-y\\hat(x)||^2 $$\n",
    "\n",
    "我们的目标就是寻找权重$w^2$,...,$w^L$ 以及偏差$b^2,...,b^L$ 来最小化目标函数\n",
    "\n",
    "### 激励函数\n",
    "\n",
    "$$\\sigma(z) \\equiv \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "\n",
    "### Feed forward \n",
    "\n",
    "+ $a=x$\n",
    "+ $a^`=\\sigma(w^lx+b^l),l=2,...,L$, $a=a^`$\n",
    "+ $y\\hat(x)=a$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络实现\n",
    "\n",
    "### 梯度下降\n",
    "\n",
    "对每一个训练数据$(x; y)$, 我们计算权重与偏差如何影响error的变化:\n",
    "+ $\\partial_E/\\partial_w$ 网络中的偏差\n",
    "+ $\\partial_E/\\partial_b$ 网络中的权重 \n",
    "\n",
    "通过减去这一变化来更新权重与偏差\n",
    "\n",
    "+ $w^`=w-\\eta\\partial_E/\\partial_w,w=w^`$\n",
    "+ $b^`=b-\\eta\\partial_E/\\partial_b,b=b^`$\n",
    "\n",
    "$\\eta$为学习速度\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "反向传播其实是对权重和偏差变化影响代价函数过程的理解。最终极的含义其实就是计算偏导数 $\\partial C/\\partial w_{jk}^l$ 和 $\\partial C/\\partial b_j^l$。但是为了计算这些值，我们首先引入一个中间量，$\\delta_j^l$，这个我们称为在 $l^{th}$ 层第 $j^{th}$ 个神经元上的误差（error）。\n",
    "\n",
    "反向传播将给出计算误差 $\\delta_j^l$ 的流程，然后将其关联到计算 $\\partial C/\\partial w_{jk}^l$ 和 $\\partial C/\\partial b_j^l$ 上。\n",
    "\n",
    "为了理解误差是如何定义的，假设在神经网络上有一个恶魔：\n",
    "\n",
    "![](./img/nn-error.png)\n",
    "\n",
    "这个小精灵在 $l$ 层的第 $j^{th}$ 个神经元上。当输入进来时，精灵对神经元的操作进行搅局。他会增加很小的变化 $\\Delta z_j^l$ 在神经元的带权输入上，使得神经元输出由 $\\sigma(z_j^l)$ 变成 $\\sigma(z_j^l + \\Delta z_j^l)$。这个变化会向网络后面的层进行传播，最终导致整个代价函数产生 $\\frac{\\partial C}{\\partial z_j^l} \\Delta z_j^l$ 的改变。\n",
    "\n",
    "现在，这个精灵变好了，试着帮助你来优化代价函数，他们试着找到可以让代价更小的 $\\Delta z_j^l$。假设 $\\frac{\\partial C}{\\partial z_j^l}$ 有一个很大的值（或正或负）。那么这个精灵可以降低代价通过选择与 $\\frac{\\partial C}{\\partial z_j^l}$ 相反符号的 $\\Delta z_j^l$ 。相反，如果$\\frac{\\partial C}{\\partial z_j^l}$ 接近 $0$，那么精灵并不能通过扰动带权输入 $z_j^l$ 来改变太多代价函数。在小精灵看来，这时候神经元已经很接近最优了\n",
    "\n",
    "![](./img/bp.png)\n",
    "\n",
    "+ BP1: 右式第一个项 $\\partial C/\\partial a_j^L$ 表示代价随着 $j^{th}$ 输出激活值的变化而变化的速度。假如 $C$ 不太依赖一个特定的输出神经元 $j$，那么$\\delta_j^L$ 就会很小，这也是我们想要的效果。右式第二项 $\\sigma'(z_j^L)$ 刻画了在 $z_j^L$ 处激活函数 $\\sigma$ 变化的速度。  \n",
    "+ BP2: 使用下一层的误差 $\\delta^{l+1}$ 来表示当前层的误差 $\\delta_l$.其中 $(w^{l+1})^T$ 是 $(l+1)^{th}$ 权重矩阵 $w^{l+1}$ 的转置。这其实可以很直觉地看做是在 $l^{th}$ 层的输出的误差的反向传播，给出了某种关于误差的度量方式。然后，我们进行 Hadamard 乘积运算 $\\odot \\sigma'(z^l)$。这会让误差通过 $l$ 层的激活函数反向传递回来并给出在第 $l$ 层的带权输入的误差 $\\delta$。通过组合(BP1)和(BP2)，我们可以计算任何层的误差了  \n",
    "+ BP3: 代价函数关于网络中任意偏差的改变率.这其实是，误差 $\\delta_j^l$ 和偏导数值 $\\partial C/\\partial b_j^l$完全一致\n",
    "+ BP4:代价函数关于任何一个权重的改变率.这告诉我们如何计算偏导数 $\\partial C/\\partial w_{jk}^l$，其中 $\\delta^l$ $a^{l-1}$ 这些量我们都已经知道如何计算了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
