{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型\n",
    "\n",
    "### 回归\n",
    "\n",
    "+ 线性回归模型原理\n",
    "+ 单变量线性回归\n",
    "+ 多变量线性回归\n",
    "+ 多项式回归\n",
    "+ Ridge\n",
    "+ LASSO\n",
    "\n",
    "### 分类\n",
    "+ Logistical Regression\n",
    "+ Linear SVM\n",
    "\n",
    "## 线性模型参考资料\n",
    "+ An Introduction to Statistical Learning chp3,chp4,chp6/chp9\n",
    "+ Elements of Statistical learning chp3,chp4/chp12\n",
    "+ 机器学习 周志华 p53~69, p121-p145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型问题\n",
    "\n",
    "对于给定的m个样本\n",
    "\n",
    "$$\\left\\{ \\left( {{x}^{\\left( 1 \\right)}},{{y}^{\\left( 1 \\right)}} \\right),\\left( {{x}^{\\left( 2 \\right)}},{{y}^{\\left( 2 \\right)}} \\right),...,\\left( {{x}^{\\left( i \\right)}},{{y}^{\\left( i \\right)}} \\right),...,\\left( {{x}^{\\left( m \\right)}},{{y}^{\\left( m \\right)}} \\right) \\right\\}$$ \n",
    "\n",
    "其中${{x}^{\\left( i \\right)}}\\in {{\\mathbb{R}}^{n}}$代表第 i个样本，为n维列向量，即${{x}^{\\left( i \\right)}}={{\\left[ \\begin{matrix}x_{1}^{\\left( i \\right)}, & x_{2}^{\\left( i \\right)}, & x_{3}^{\\left( i \\right)}, & \\cdots  & ,x_{n}^{\\left( i \\right)}  \\\\ \\end{matrix} \\right]}^{T}}$ (这里为了方便对${{x}^{\\left( i \\right)}}$ 进行了增广，即对n-1维的变量增加取值为1的一维，组成n维，这样可以方便地将公式写成向量形式，实际就是加了一个常量），\n",
    "\n",
    "${{y}^{\\left( i \\right)}}\\in {{\\mathbb{R}}^{1}}$为对应的函数值。\n",
    "\n",
    "线性回归的目标是找到自变量和函数值之间的一个最佳线性拟合${{h}_{\\theta }}\\left( x \\right)={{\\theta }^{T}}x$\n",
    "，其中 $\\theta \\in {{\\mathbb{R}}^{n}}$ ，表示为n维列向量，即$\\theta ={{\\left[ \\begin{matrix}{{\\theta }_{1}}, & {{\\theta }_{2}}, & {{\\theta }_{3}}, & \\cdots  & ,{{\\theta }_{n}}  \\\\ \\end{matrix} \\right]}^{T}}$，我们要找到一个$\\theta$使得这种拟合是最优的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题求解\n",
    "\n",
    "为了找到最优的$\\theta $，需要首先定义一个目标函数来说明什么样的$\\theta $是最优的。线性回归采用的是最小平方误差准则,因此目标函数为：\n",
    "\n",
    "$$\\ell \\left( \\theta  \\right)=\\frac{1}{2}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}_{\\theta }}\\left( {{x}^{\\left( i \\right)}} \\right)-{{y}^{\\left( i \\right)}} \\right)}^{2}}}=\\frac{1}{2}\\sum\\limits_{i=1}^{m}{{{\\left( {{\\theta }^{T}}{{x}^{\\left( i \\right)}}-{{y}^{\\left( i \\right)}} \\right)}^{2}}}$$\n",
    "\n",
    "求和式内部的每一项为预测值和原始值的平方误差差，所以目标函数定义了所以样本的预测值和原始值平方差的和，要使得线性拟合最优，那么就需要选取$\\theta$使得这个函数取值最小。求解$\\theta $是一个优化问题，可以用不同的优化方法来做，比如梯度下降或者牛顿法等等，当然能直接得到解析解最好了，对于现在这个问题，我们可以很方便求出它的解析解。\n",
    "\n",
    "我们将目标函数写成向量形式\n",
    "\n",
    "$$\\begin{align} X=\\left[ \\begin{matrix}{{x}^{\\left( 1 \\right)T}}  \\\\{{x}^{\\left( 2 \\right)T}}  \\\\{{x}^{\\left( 3 \\right)T}}  \\\\ \\vdots   \\\\{{x}^{\\left( m \\right)T}}  \\\\ \\end{matrix} \\right], y=\\left[ \\begin{matrix} {{y}^{\\left( 1 \\right)}}  \\\\{{y}^{\\left( 2 \\right)}}  \\\\{{y}^{\\left( 3 \\right)}}  \\\\\\vdots   \\\\{{y}^{\\left( m \\right)}}  \\\\ \\end{matrix} \\right], \\theta =\\left[ \\begin{matrix}{{\\theta }_{1}}  \\\\{{\\theta }_{2}}  \\\\{{\\theta }_{3}}  \\\\ \\vdots   \\\\{{\\theta }_{n}}  \\\\\\end{matrix} \\right] \\end{align}$$\n",
    "\n",
    "这样目标函数\n",
    "$$\\begin{align}\\ell (\\theta )&=\\frac{1}{2}{{(X\\theta -y)}^{T}}(X\\theta -y) \\\\& =\\frac{1}{2}({{\\theta }^{T}}{{X}^{T}}-{{y}^{T}})(X\\theta -y) \\\\& =\\frac{1}{2}({{\\theta }^{T}}{{X}^{T}}X\\theta -2{{y}^{T}}X\\theta -{{y}^{T}}y) \\end{align}$$\n",
    "为了求目标函数极值，对其求导：\n",
    "$$\\frac{d\\ell (\\theta )}{d\\theta }=\\frac{d\\frac{1}{2}({{\\theta }^{T}}{{X}^{T}}X\\theta -2{{y}^{T}}X\\theta +{{y}^{T}}y)}{d\\theta }={{X}^{T}}X\\theta -{{X}^{T}}y$$\n",
    "（上式利用了求导公式$\\frac{d({{x}^{T}}Ax)}{dx}=2Ax$和$\\frac{d({{b}^{T}}x)}{dx}=b$，令\n",
    "$A={{X}^{T}}X$，${{b}^{T}}={{y}^{T}}X$)\n",
    "令导数为0：\n",
    "$$\\begin{align}&&{{X}^{T}}X\\theta -{{X}^{T}}y&=0 \\\\ &\\Rightarrow &{{X}^{T}}X\\theta &={{X}^{T}}y \\\\ &\\Rightarrow &\\theta &={{({{X}^{T}}X)}^{-1}}{{X}^{T}}y \\\\\\end{align}$$\n",
    "这样便求出了最优的参数$\\theta $，回归曲线${{h}_{\\theta }}(x)$便也求出了。\n",
    "\n",
    " 对于${{X}^{T}}X\\theta ={{X}^{T}}y\\Rightarrow \\theta ={{({{X}^{T}}X)}^{-1}}{{X}^{T}}y$这一步推导 ，需要${{X}^{T}}X$可逆，那么它可逆吗？如果问题（1）中${{X}^{T}}X$正定，那么正定矩阵是可逆的，就不存在这个问题了。但如果如${{X}^{T}}X$是半正定，那么能说明它可逆吗？\n",
    "在实际中，如果${{X}^{T}}X$不可逆，为了得到一个解，我们可以用它的伪逆当作逆来用，这样做在数学上也可以证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率模型\n",
    "为什么线性回归的目标函数为何是最小二乘？\n",
    "对于一个样本的自变量${{x}^{\\left( i \\right)}}$，线性回归对其函数值预测值为${{h}_{\\theta }}({{x}^{(i)}})$，而它的真实值为${{y}^{\\left( i \\right)}}$，两者之间是有一定偏差的。假设这个偏差$\\xi$本身是服从均值为0，方差为${{\\sigma }^{2}}$的正态分布的，即$\\xi \\sim N(0,{{\\sigma }^{2}})$。\n",
    "那么由于${{y}^{\\left( i \\right)}}={{h}_{\\theta }}({{x}^{\\left( i \\right)}})+\\xi$，所以${{y}^{(i)}}\\sim N({{h}_{\\theta }}({{x}^{(i)}}),{{\\sigma }^{2}})$，这样，根据正态分布概率密度函数\n",
    "\n",
    "$$p({{y}^{\\left( i \\right)}}|{{x}^{\\left( i \\right)}};\\theta )=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\exp (-\\frac{{{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}}{2{{\\sigma }^{2}}})=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\exp (-\\frac{{{({{\\theta }^{T}}{{x}^{(i)}}-{{y}^{(i)}})}^{2}}}{2{{\\sigma }^{2}}})$$\n",
    "\n",
    "现在，可以写出对参数$\\theta$估计的对数似然函数了\n",
    "\n",
    "$$\\begin{align}L(\\theta )&=\\log \\prod\\limits_{i=1}^{m}{p({{y}^{(i)}}|{{x}^{(i)}};\\theta )} \\\\& =\\sum\\limits_{i=1}^{m}{\\log (\\frac{1}{\\sqrt{2\\pi }\\sigma }\\exp (-\\frac{{{({{\\theta }^{T}}{{x}^{(i)}}-{{y}^{(i)}})}^{2}}}{2{{\\sigma }^{2}}})}) \\\\& =\\sum\\limits_{i=1}^{m}{(\\log \\frac{1}{\\sqrt{2\\pi }\\sigma }-\\frac{{{({{\\theta }^{T}}{{x}^{(i)}}-{{y}^{(i)}})}^{2}}}{2{{\\sigma }^{2}}}}) \\end{align}$$\n",
    "\n",
    "为了求解最优参数$\\theta$，需要对似然函数最大化，而对上面似然函数的最大化就等价与对$\\sum\\limits_{i=1}^{m}{{{\\left( {{\\theta }^{T}}{{x}^{\\left( i \\right)}}-{{y}^{\\left( i \\right)}} \\right)}^{2}}}$最小化，这和我们开始的最小二次的目标是一致的，这就从概率模型角度解释了线性回归采用最小二次作为目标的合理性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
